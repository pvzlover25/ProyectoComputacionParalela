{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RkrO0WFdLut",
        "outputId": "44a2663f-566d-496c-f7ca-1281b045ffa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        target. The options -dlto -arch=sm_NN will add a lto_NN target; if you want\n",
            "        to only add a lto_NN target and not the compute_NN that -arch=sm_NN usually\n",
            "        for '--gpu-architecture' may be a 'real' architecture (such as a sm_50),\n",
            "        --gpu-architecture=sm_50' is equivalent to 'nvcc --gpu-architecture=compute_50\n",
            "        --gpu-code=sm_50,compute_50'.\n",
            "        -arch=all         build for all supported architectures (sm_*), and add PTX\n",
            "        -arch=all-major   build for just supported major versions (sm_*0), plus the\n",
            "        -arch=native      build for all architectures (sm_*) on the current system\n",
            "        'native','sm_50','sm_52','sm_53','sm_60','sm_61','sm_62','sm_70','sm_72',\n",
            "        'sm_75','sm_80','sm_86','sm_87','sm_89','sm_90','sm_90a'.\n",
            "        (such as sm_50), and PTX code for the 'virtual' architecture (such as compute_50).\n",
            "        For instance, '--gpu-architecture=compute_60' is not compatible with '--gpu-code=sm_52',\n",
            "        features that are not present on 'sm_52'.\n",
            "        'lto_75','lto_80','lto_86','lto_87','lto_89','lto_90','lto_90a','sm_50',\n",
            "        'sm_52','sm_53','sm_60','sm_61','sm_62','sm_70','sm_72','sm_75','sm_80',\n",
            "        'sm_86','sm_87','sm_89','sm_90','sm_90a'.\n",
            "        List the non-accelerated gpu architectures (sm_XX) supported by the compiler\n"
          ]
        }
      ],
      "source": [
        "!nvcc --help | grep sm_"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhj7CjJvQ6qL",
        "outputId": "4a57d8b1-e174-473f-919e-62acf69fa76d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Jul  8 22:59:47 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   63C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile bitonic_sort.cu\n",
        "#include <iostream>\n",
        "#include <chrono>\n",
        "#include <climits>\n",
        "#include <vector>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <device_launch_parameters.h>\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "void printArray(int* arr, int size){\n",
        "  for(int i=0;i<size;i++) cout<<arr[i]<<\" \";\n",
        "  cout<<\"\\n\";\n",
        "}\n",
        "\n",
        "// Kernel de Bitonic Sort\n",
        "__global__ void bitonic_sort_kernel(int* data, int j, int k, int n) {\n",
        "    unsigned int i = threadIdx.x + blockDim.x * blockIdx.x;\n",
        "    if (i >= n) return;\n",
        "\n",
        "    unsigned int ixj = i ^ j;\n",
        "    if (ixj > i && ixj < n) {\n",
        "        bool ascendente = ((i & k) == 0);\n",
        "        if ((data[i] > data[ixj]) == ascendente) {\n",
        "            int temp = data[i];\n",
        "            data[i] = data[ixj];\n",
        "            data[ixj] = temp;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "// Función para redondear al siguiente número potencia de 2\n",
        "int sigtePotenciaDos(int n) {\n",
        "    int ret = 1;\n",
        "    while (ret < n) ret <<= 1;\n",
        "    return ret;\n",
        "}\n",
        "\n",
        "void bitonic_sort(int* h_data, int n) {\n",
        "    int padded_n = sigtePotenciaDos(n);//\n",
        "    int* h_padded = new int[padded_n];\n",
        "    for (int i = 0; i < n; i++) h_padded[i] = h_data[i];\n",
        "    for (int i = n; i < padded_n; i++) h_padded[i] = INT_MAX;\n",
        "\n",
        "    int* d_data;\n",
        "    cudaMalloc(&d_data, padded_n * sizeof(int));\n",
        "    cudaMemcpy(d_data, h_padded, padded_n * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    int hebras = 512;\n",
        "    int numBloques = (padded_n + hebras - 1) / hebras;\n",
        "\n",
        "    for (int k = 2; k <= padded_n; k <<= 1) {\n",
        "        for (int j = k >> 1; j > 0; j >>= 1) {\n",
        "            bitonic_sort_kernel<<<numBloques,hebras>>>(d_data, j, k, padded_n);\n",
        "            cudaDeviceSynchronize();\n",
        "        }\n",
        "    }\n",
        "\n",
        "    cudaMemcpy(h_padded, d_data, padded_n * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "    cudaFree(d_data);\n",
        "\n",
        "    // Copiar sólo los n elementos ordenados reales\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        h_data[i] = h_padded[i];\n",
        "    }\n",
        "    delete[] h_padded;\n",
        "}\n",
        "\n",
        "int main(int argc, char** argv){\n",
        "  srand(time(NULL));\n",
        "  int size=atoi(argv[1]);\n",
        "  int* arr=new int[size];\n",
        "  for(int i=0;i<size;i++) arr[i]=1+rand()%200;\n",
        "  auto start=chrono::high_resolution_clock::now();\n",
        "  bitonic_sort(arr,size);\n",
        "  auto finish=chrono::high_resolution_clock::now();\n",
        "  auto duration=chrono::duration_cast<chrono::nanoseconds>(finish - start).count();\n",
        "  cout<<\"Tiempo demorado en ordenar arreglo: \"<<duration/1000000000.0<<\" s\\n\";\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCREl2AORFGP",
        "outputId": "93d03c2a-eac9-4568-9a0d-cb701035cb97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting bitonic_sort.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 bitonic_sort.cu -o bitonic_sort"
      ],
      "metadata": {
        "id": "x79f2JADZM_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./bitonic_sort 16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HaEujbEZh6H",
        "outputId": "8827133a-aa6a-4126-88cb-8facc941962f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arreglo original: 186 119 44 30 11 140 138 44 77 43 17 63 117 167 155 16 \n",
            "Arreglo ordenado: 11 16 17 30 43 44 44 63 77 117 119 138 140 155 167 186 \n"
          ]
        }
      ]
    }
  ]
}